{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers import LeakyReLU, Input\n",
    "from keras.optimizers import SGD, Adam\n",
    "from sklearn.utils import shuffle\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ONE HOT ENCODES A GIVEN COLUMN\n",
    "def onehot2(x): return np.array(OneHotEncoder(n_values = 2).fit_transform(x.values.reshape(-1,1)).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ONE HOT ENCODES A GIVEN COLUMN\n",
    "def onehot(x): return np.array(OneHotEncoder().fit_transform(x.values.reshape(-1,1)).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#LOAD DATA\n",
    "train_data = pandas.read_csv(\"/storage/tanel/child_age_gender/exp/ivectors_2048/train/export.csv\", sep=\" \")\n",
    "val_data = pandas.read_csv(\"/storage/tanel/child_age_gender/exp/ivectors_2048/dev/export.csv\", sep=\" \")\n",
    "test_data = pandas.read_csv(\"/storage/tanel/child_age_gender/exp/ivectors_2048/test/export.csv\", sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#PREFORMAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def format(data):\n",
    "    del data['Unnamed: 605']\n",
    "    mask = data['AgeGroup'] == 'ag1'\n",
    "    column_name = 'AgeGroup'\n",
    "    data.loc[mask, column_name] = 0\n",
    "    mask = data['AgeGroup'] == 'ag2'\n",
    "    column_name = 'AgeGroup'\n",
    "    data.loc[mask, column_name] = 1\n",
    "    mask = data['AgeGroup'] == 'ag3'\n",
    "    column_name = 'AgeGroup'\n",
    "    data.loc[mask, column_name] = 2\n",
    "    mask = data['Gender'] == 'm'\n",
    "    column_name = 'Gender'\n",
    "    data.loc[mask, column_name] = 0\n",
    "    mask = data['Gender'] == 'f'\n",
    "    column_name = 'Gender'\n",
    "    data.loc[mask, column_name] = 1\n",
    "    return data\n",
    "\n",
    "train_data = format(train_data)\n",
    "val_data = format(val_data)\n",
    "test_data = format(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_labels_age_group = onehot(train_data['AgeGroup'])\n",
    "val_labels_age_group = onehot(val_data['AgeGroup'])\n",
    "test_labels_age_group = onehot(test_data['AgeGroup'])\n",
    "\n",
    "train_labels_age = train_data['Age']\n",
    "val_labels_age = val_data['Age']\n",
    "test_labels_age = test_data['Age']\n",
    "\n",
    "train_labels_gender =  onehot(train_data['Gender'])\n",
    "val_labels_gender = onehot(val_data['Gender'])\n",
    "test_labels_gender = onehot(test_data['Gender'])\n",
    "\n",
    "train_i_vectors = train_data.iloc[:, 5:].as_matrix()\n",
    "val_i_vectors = val_data.iloc[:, 5:].as_matrix()\n",
    "test_i_vectors = test_data.iloc[:, 5:].as_matrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     70\n",
       "unique     1\n",
       "top        0\n",
       "freq      70\n",
       "Name: Gender, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#GENDER MODEL\n",
    "### 0.8829"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gender_model = Sequential([\n",
    "    Dense(4000, activation='relu', input_shape=(600,)),\n",
    "    Dropout(0.5),\n",
    "    Dense(2000, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(2, activation='softmax')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.4,\n",
    "                              patience=1, min_lr=0.0001, verbose=1)\n",
    "gender_model.compile(loss='categorical_crossentropy', optimizer=SGD(0.001), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17428 samples, validate on 2100 samples\n",
      "Epoch 1/10\n",
      "17428/17428 [==============================] - 9s 543us/step - loss: 0.7195 - acc: 0.5716 - val_loss: 0.6608 - val_acc: 0.5814\n",
      "Epoch 2/10\n",
      "17428/17428 [==============================] - 6s 362us/step - loss: 0.6421 - acc: 0.6420 - val_loss: 0.6036 - val_acc: 0.7300\n",
      "Epoch 3/10\n",
      "11072/17428 [==================>...........] - ETA: 2s - loss: 0.5946 - acc: 0.6898"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-bb7f65dd71e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_i_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels_gender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                     batch_size=32)\n\u001b[0m",
      "\u001b[0;32m/home/hpc_lkpiel/anaconda3/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/home/hpc_lkpiel/anaconda3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/home/hpc_lkpiel/anaconda3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hpc_lkpiel/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hpc_lkpiel/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hpc_lkpiel/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hpc_lkpiel/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/hpc_lkpiel/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hpc_lkpiel/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gender_model.fit(x=train_i_vectors,\n",
    "                    y=train_labels_gender,\n",
    "                    validation_data=(val_i_vectors, val_labels_gender),\n",
    "                    epochs=10,\n",
    "                    batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_i_vectors_gender_19' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-b470fb7635af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgender_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_i_vectors_gender_19\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels_gender_19\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'val_i_vectors_gender_19' is not defined"
     ]
    }
   ],
   "source": [
    "gender_model.evaluate(val_i_vectors_gender_19, val_labels_gender_19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 0s 176us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0284677137487701, 1.0]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_model.evaluate(test_i_vectors_gender_18, test_labels_gender_18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    17428.000000\n",
       "mean        13.638685\n",
       "std          2.331605\n",
       "min          9.000000\n",
       "25%         12.000000\n",
       "50%         14.000000\n",
       "75%         15.000000\n",
       "max         19.000000\n",
       "Name: Age, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#AGE GROUP WITH KNOWN GENDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_males = train_data[train_data['Gender'] == 0]\n",
    "train_data_females = train_data[train_data['Gender'] == 1]\n",
    "val_data_males = val_data[val_data['Gender'] == 0]\n",
    "val_data_females = val_data[val_data['Gender'] == 1]\n",
    "\n",
    "\n",
    "train_labels_males =  onehot(train_data_males['AgeGroup'])\n",
    "val_labels_males = onehot(val_data_males['AgeGroup'])\n",
    "\n",
    "train_i_vectors_males = train_data_males.iloc[:, 5:].as_matrix()\n",
    "val_i_vectors_males = val_data_males.iloc[:, 5:].as_matrix()\n",
    "\n",
    "train_labels_females =  onehot(train_data_females['AgeGroup'])\n",
    "val_labels_females = onehot(val_data_females['AgeGroup'])\n",
    "\n",
    "train_i_vectors_females = train_data_females.iloc[:, 5:].as_matrix()\n",
    "val_i_vectors_females = val_data_females.iloc[:, 5:].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MALES\n",
    "### AGE GROUP 0.8345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "age_group_model_with_males = Sequential([\n",
    "    Dense(4000, activation='relu', input_shape=(600,)),\n",
    "    Dropout(0.8),\n",
    "    Dense(1000, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(3, activation='softmax')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "age_group_model_with_males.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7140 samples, validate on 1190 samples\n",
      "Epoch 1/10\n",
      "7140/7140 [==============================] - 2s 305us/step - loss: 1.1461 - acc: 0.4629 - val_loss: 0.8088 - val_acc: 0.7924\n",
      "Epoch 2/10\n",
      "7140/7140 [==============================] - 2s 284us/step - loss: 0.8486 - acc: 0.6141 - val_loss: 0.7158 - val_acc: 0.7824\n",
      "Epoch 3/10\n",
      "5600/7140 [======================>.......] - ETA: 0s - loss: 0.6770 - acc: 0.6996"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-cf6d2f59edc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_i_vectors_males\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels_males\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                     batch_size=32)\n\u001b[0m",
      "\u001b[0;32m/home/hpc_lkpiel/anaconda3/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/home/hpc_lkpiel/anaconda3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/home/hpc_lkpiel/anaconda3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hpc_lkpiel/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hpc_lkpiel/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hpc_lkpiel/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hpc_lkpiel/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/hpc_lkpiel/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hpc_lkpiel/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "    age_group_model_with_males.fit(x=train_i_vectors_males,\n",
    "                        y=train_labels_males,\n",
    "                        validation_data=(val_i_vectors_males, val_labels_males),\n",
    "                        epochs=10,\n",
    "                        batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "č"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#FEMALES\n",
    "### 0.7363"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "age_group_model_with_females = Sequential([\n",
    "    Dense(4000, activation='relu', input_shape=(600,)),\n",
    "    Dropout(0.8),\n",
    "    Dense(1000, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(3, activation='softmax')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "age_group_model_with_females.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'], callback=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "age_group_model_with_females.fit(x=train_i_vectors_females,\n",
    "                    y=train_labels_females,\n",
    "                    validation_data=(val_i_vectors_females, val_labels_females),\n",
    "                    epochs=10,\n",
    "                    batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MULTI OUTPUT GENDER AGE-GROUP\n",
    "\n",
    "### GROUP 0.787\n",
    "### GENDER 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(600,))\n",
    "x = Dense(4000, activation='relu')(input_layer)\n",
    "x = Dropout(0.7)(x)\n",
    "x = Dense(1500, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(300, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "output_layer_1 = Dense(3, activation='softmax', name='group_output')(x)\n",
    "output_layer_2 =  Dense(2, activation='softmax', name='gender_output')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "multi_model = Model(inputs=input_layer, outputs=[output_layer_1, output_layer_2])\n",
    "\n",
    "multi_model.compile(loss={'group_output':'categorical_crossentropy', 'gender_output':'categorical_crossentropy'},\n",
    "                    optimizer='sgd',\n",
    "                    metrics={'group_output':'accuracy','gender_output':'accuracy'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_group_output_loss', factor=0.8,\n",
    "                              patience=1, min_lr=0.0001, verbose=1)\n",
    "multi_model.fit(train_i_vectors,\n",
    "                [train_labels_age_group, train_labels_gender],\n",
    "                validation_data=(val_i_vectors, [val_labels_age_group, val_labels_gender]),\n",
    "                epochs=20,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#AGE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "from keras import backend as K\n",
    "def age_group_accuracy(y_true, y_pred):\n",
    "    array = np.array([0]*13 + [1]*2 + [2]*10000000)\n",
    "    age_to_group = theano.shared(array)\n",
    "    ages_true = age_to_group[(theano.tensor.iround(y_true))]\n",
    "    ages_pred = age_to_group[(theano.tensor.iround(y_pred))] \n",
    "    return K.mean(K.equal(ages_true, ages_pred), axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "age_model = Sequential([\n",
    "   \n",
    "        Dense(2000, activation = 'relu', input_shape=(600,)),\n",
    "    Dropout(0.7),\n",
    "    Dense(600, activation = 'relu', input_shape=(600,)),\n",
    "    Dropout(0.4),\n",
    "    Dense(100, activation = 'sigmoid'),\n",
    "    Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "age_model.compile(loss='mse',\n",
    "                  optimizer=SGD(lr=0.01),\n",
    "                  metrics=[age_group_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "age_model.fit(x=train_i_vectors,\n",
    "              y=train_labels_age,\n",
    "              validation_data=(val_i_vectors,val_labels_age),\n",
    "              epochs=50,\n",
    "              batch_size=32,\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# AGE GROUP MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "age_group_model = Sequential([\n",
    "    Dense(2000, activation='relu', input_shape=(600,)),\n",
    "    Dropout(0.6),\n",
    "    Dense(2000, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(2000, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(3, activation='softmax')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.4,\n",
    "                              patience=1, min_lr=0.0001, verbose=1)\n",
    "age_group_model.compile(loss='categorical_crossentropy', optimizer=SGD(0.01), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17428 samples, validate on 2100 samples\n",
      "Epoch 1/2\n",
      "17428/17428 [==============================] - 6s 365us/step - loss: 0.4831 - acc: 0.7978 - val_loss: 0.6891 - val_acc: 0.7729\n",
      "Epoch 2/2\n",
      "17428/17428 [==============================] - 6s 363us/step - loss: 0.4425 - acc: 0.8162 - val_loss: 0.6989 - val_acc: 0.7681\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f69dbc80b70>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_group_model.fit(x=train_i_vectors,\n",
    "                    y=train_labels_age_group,\n",
    "                    validation_data=(val_i_vectors, val_labels_age_group),\n",
    "                    epochs=2,\n",
    "                    batch_size=32,\n",
    "                    callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100/2100 [==============================] - 0s 99us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.65961559846287687, 0.71333333321980064]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MULTI OUTPUT AGE GROUP-AGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(600,))\n",
    "x = Dense(4000, activation='relu')(input_layer)\n",
    "x = Dropout(0.7)(x)\n",
    "x = Dense(300, activation='sigmoid')(x)\n",
    "\n",
    "output_layer_1 = Dense(3, activation='softmax', name='group_output')(x)\n",
    "output_layer_2 =  Dense(1, name='age_output')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "multi_model = Model(inputs=input_layer, outputs=[output_layer_1, output_layer_2])\n",
    "\n",
    "multi_model.compile(loss={'group_output':'categorical_crossentropy', 'age_output':'mse'},\n",
    "                    optimizer=SGD(0.01),\n",
    "                    metrics={'group_output':'accuracy','age_output':age_group_accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_group_output_loss', factor=0.8,\n",
    "                              patience=1, min_lr=0.0001, verbose=1)\n",
    "multi_model.fit(train_i_vectors,\n",
    "                [train_labels_age_group, train_labels_age],\n",
    "                validation_data=(val_i_vectors, [val_labels_age_group, val_labels_age]),\n",
    "                epochs=20,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MULTI OUTPUT AGE-GENDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Refreshing lock /tmp/hpc_lkpiel/theano.NOBACKUP/compiledir_Linux-3.14.28-x86_64-with-redhat-6.7-Carbon-x86_64-3.5.2-64/lock_dir/lock\n"
     ]
    }
   ],
   "source": [
    "input_layer = Input(shape=(600,))\n",
    "x = Dense(4000, activation='relu')(input_layer)\n",
    "x = Dropout(0.7)(x)\n",
    "x = Dense(500, activation='relu')(input_layer)\n",
    "x = Dropout(0.4)(x)\n",
    "x = Dense(200, activation='sigmoid')(x)\n",
    "\n",
    "output_layer_1 = Dense(2, activation='softmax', name='gender_output')(x)\n",
    "output_layer_2 =  Dense(1, name='age_output')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'age_group_accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-af302a191571>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m multi_model.compile(loss={'gender_output':'categorical_crossentropy', 'age_output':'mse'},\n\u001b[1;32m      4\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                     metrics={'gender_output':'accuracy','age_output':age_group_accuracy})\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'age_group_accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "multi_model = Model(inputs=input_layer, outputs=[output_layer_1, output_layer_2])\n",
    "\n",
    "multi_model.compile(loss={'gender_output':'categorical_crossentropy', 'age_output':'mse'},\n",
    "                    optimizer=SGD(0.001),\n",
    "                    metrics={'gender_output':'accuracy','age_output':age_group_accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "multi_model.fit(train_i_vectors,\n",
    "                [train_labels_gender, train_labels_age],\n",
    "                validation_data=(val_i_vectors, [val_labels_gender, val_labels_age]),\n",
    "                epochs=100,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lebo = train_data.query('Age == 10 & Gender == 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     700\n",
       "unique      1\n",
       "top         0\n",
       "freq      700\n",
       "Name: Gender, dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lebo['Gender'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### AGE MODEL 9\n",
    "val_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(10, 18):\n",
    "    exec(\"train_data_gender_%d = train_data[train_data['Age'] == %d]\" % (i, i))\n",
    "    exec(\"val_data_gender_%d = val_data[val_data['Age'] == %d]\" % (i, i))\n",
    "    exec(\"test_data_gender_%d = test_data[test_data['Age'] == %d]\" % (i, i))\n",
    "    exec(\"train_labels_gender_%d = onehot2(train_data_gender_%d['Gender'])\" % (i, i))\n",
    "    exec(\"val_labels_gender_%d = onehot2(val_data_gender_%d['Gender'])\" % (i, i))\n",
    "    exec(\"test_labels_gender_%d = onehot2(test_data_gender_%d['Gender'])\" % (i, i))\n",
    "    exec(\"train_i_vectors_gender_%d = train_data_gender_%d.iloc[:, 5:].as_matrix()\" % (i,i))\n",
    "    exec(\"val_i_vectors_gender_%d =  val_data_gender_%d.iloc[:, 5:].as_matrix()\" % (i,i))\n",
    "    exec(\"test_i_vectors_gender_%d =  test_data_gender_%d.iloc[:, 5:].as_matrix()\" % (i,i))\n",
    "\n",
    "    exec(\"train_data_gender_%d_boys = train_data[train_data['Age'] == %d]\" % (i, i))\n",
    "    exec(\"val_data_gender_%d_boys = val_data[val_data['Age'] == %d]\" % (i, i))\n",
    "    exec(\"test_data_gender_%d_boys = test_data[test_data['Age'] == %d]\" % (i, i))\n",
    "    exec(\"train_labels_gender_%d_boys = onehot2(train_data_gender_%d_boys['Gender'])\" % (i, i))\n",
    "    exec(\"val_labels_gender_%d_boys = onehot2(val_data_gender_%d_boys['Gender'])\" % (i, i))\n",
    "    exec(\"test_labels_gender_%d_boys = onehot2(test_data_gender_%d_boys['Gender'])\" % (i, i))\n",
    "    exec(\"train_i_vectors_gender_%d_boys = train_data_gender_%d_boys.iloc[:, 5:].as_matrix()\" % (i, i))\n",
    "    exec(\"val_i_vectors_gender_%d_boys =  val_data_gender_%d_boys.iloc[:, 5:].as_matrix()\" % (i, i))\n",
    "    exec(\"test_i_vectors_gender_%d_boys =  test_data_gender_%d_boys.iloc[:, 5:].as_matrix()\" % (i, i))\n",
    "\n",
    "    \n",
    "    exec(\"train_data_gender_%d_girls = train_data[train_data['Age'] == %d]\" % (i, i))\n",
    "    exec(\"val_data_gender_%d_girls = val_data[val_data['Age'] == %d]\" % (i, i))\n",
    "    exec(\"test_data_gender_%d_girls = test_data[test_data['Age'] == %d]\" % (i, i))\n",
    "    exec(\"train_labels_gender_%d_girls = onehot2(train_data_gender_%d_girls['Gender'])\" % (i, i))\n",
    "    exec(\"val_labels_gender_%d_girls = onehot2(val_data_gender_%d_girls['Gender'])\" % (i, i))\n",
    "    exec(\"test_labels_gender_%d_girls = onehot2(test_data_gender_%d_girls['Gender'])\" % (i, i))\n",
    "    exec(\"train_i_vectors_gender_%d_girls = train_data_gender_%d_girls.iloc[:, 5:].as_matrix()\" % (i, i))\n",
    "    exec(\"val_i_vectors_gender_%d_girls =  val_data_gender_%d_girls.iloc[:, 5:].as_matrix()\" % (i, i))\n",
    "    exec(\"test_i_vectors_gender_%d_girls =  test_data_gender_%d_girls.iloc[:, 5:].as_matrix()\" % (i, i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>AgeGroup</th>\n",
       "      <th>ivec1</th>\n",
       "      <th>ivec2</th>\n",
       "      <th>ivec3</th>\n",
       "      <th>ivec4</th>\n",
       "      <th>ivec5</th>\n",
       "      <th>...</th>\n",
       "      <th>ivec591</th>\n",
       "      <th>ivec592</th>\n",
       "      <th>ivec593</th>\n",
       "      <th>ivec594</th>\n",
       "      <th>ivec595</th>\n",
       "      <th>ivec596</th>\n",
       "      <th>ivec597</th>\n",
       "      <th>ivec598</th>\n",
       "      <th>ivec599</th>\n",
       "      <th>ivec600</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>143_10001143_100_ch1</td>\n",
       "      <td>143_1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.418398</td>\n",
       "      <td>-0.938296</td>\n",
       "      <td>0.669691</td>\n",
       "      <td>-1.463043</td>\n",
       "      <td>0.453503</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128172</td>\n",
       "      <td>0.934197</td>\n",
       "      <td>0.765379</td>\n",
       "      <td>-0.335819</td>\n",
       "      <td>0.070354</td>\n",
       "      <td>0.296477</td>\n",
       "      <td>1.252707</td>\n",
       "      <td>1.078482</td>\n",
       "      <td>-0.801526</td>\n",
       "      <td>0.871624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>143_10001143_10_ch1</td>\n",
       "      <td>143_1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.633367</td>\n",
       "      <td>-0.525246</td>\n",
       "      <td>0.424735</td>\n",
       "      <td>-1.210979</td>\n",
       "      <td>1.081761</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.630734</td>\n",
       "      <td>-0.029553</td>\n",
       "      <td>-0.920571</td>\n",
       "      <td>0.509557</td>\n",
       "      <td>-0.864417</td>\n",
       "      <td>-0.295759</td>\n",
       "      <td>0.456390</td>\n",
       "      <td>0.789470</td>\n",
       "      <td>0.396610</td>\n",
       "      <td>0.723979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>143_10001143_129_ch1</td>\n",
       "      <td>143_1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.066284</td>\n",
       "      <td>0.357609</td>\n",
       "      <td>-0.992035</td>\n",
       "      <td>-2.905210</td>\n",
       "      <td>0.573784</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.894630</td>\n",
       "      <td>0.442736</td>\n",
       "      <td>0.312838</td>\n",
       "      <td>0.649240</td>\n",
       "      <td>-0.143712</td>\n",
       "      <td>0.121022</td>\n",
       "      <td>-0.974557</td>\n",
       "      <td>0.138829</td>\n",
       "      <td>-0.163264</td>\n",
       "      <td>-0.292201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>143_10001143_1432_ch1</td>\n",
       "      <td>143_1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.592274</td>\n",
       "      <td>-0.974527</td>\n",
       "      <td>0.520979</td>\n",
       "      <td>-0.997169</td>\n",
       "      <td>0.200358</td>\n",
       "      <td>...</td>\n",
       "      <td>0.966990</td>\n",
       "      <td>-1.037654</td>\n",
       "      <td>-0.314220</td>\n",
       "      <td>0.070033</td>\n",
       "      <td>1.538839</td>\n",
       "      <td>0.406294</td>\n",
       "      <td>-0.334310</td>\n",
       "      <td>-0.633877</td>\n",
       "      <td>0.171371</td>\n",
       "      <td>0.854166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>143_10001143_1433_ch1</td>\n",
       "      <td>143_1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.050412</td>\n",
       "      <td>-0.750296</td>\n",
       "      <td>0.077094</td>\n",
       "      <td>-1.177778</td>\n",
       "      <td>0.081897</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109936</td>\n",
       "      <td>0.118326</td>\n",
       "      <td>1.261557</td>\n",
       "      <td>0.046693</td>\n",
       "      <td>-0.327969</td>\n",
       "      <td>1.091086</td>\n",
       "      <td>-0.039490</td>\n",
       "      <td>0.975986</td>\n",
       "      <td>1.139612</td>\n",
       "      <td>-0.202748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>143_10001143_1434_ch1</td>\n",
       "      <td>143_1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.627047</td>\n",
       "      <td>-0.383151</td>\n",
       "      <td>-0.159873</td>\n",
       "      <td>-1.425629</td>\n",
       "      <td>-0.476528</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.311192</td>\n",
       "      <td>0.765314</td>\n",
       "      <td>0.593452</td>\n",
       "      <td>0.092819</td>\n",
       "      <td>0.672037</td>\n",
       "      <td>0.594454</td>\n",
       "      <td>0.236099</td>\n",
       "      <td>0.895553</td>\n",
       "      <td>0.071543</td>\n",
       "      <td>0.121591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>143_10001143_1435_ch1</td>\n",
       "      <td>143_1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.078822</td>\n",
       "      <td>-0.456977</td>\n",
       "      <td>0.876714</td>\n",
       "      <td>-0.795689</td>\n",
       "      <td>0.072347</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.540425</td>\n",
       "      <td>0.654016</td>\n",
       "      <td>0.735474</td>\n",
       "      <td>1.460004</td>\n",
       "      <td>0.566635</td>\n",
       "      <td>0.684433</td>\n",
       "      <td>-0.643174</td>\n",
       "      <td>0.192285</td>\n",
       "      <td>-0.072796</td>\n",
       "      <td>0.381613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>143_10001143_157_ch1</td>\n",
       "      <td>143_1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.300843</td>\n",
       "      <td>-0.825967</td>\n",
       "      <td>0.687509</td>\n",
       "      <td>-1.794576</td>\n",
       "      <td>-0.476436</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.761839</td>\n",
       "      <td>-0.776795</td>\n",
       "      <td>0.038089</td>\n",
       "      <td>-0.631255</td>\n",
       "      <td>-0.029913</td>\n",
       "      <td>-0.126166</td>\n",
       "      <td>1.100268</td>\n",
       "      <td>-0.858795</td>\n",
       "      <td>0.039926</td>\n",
       "      <td>-1.009529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>143_10001143_158_ch1</td>\n",
       "      <td>143_1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.177312</td>\n",
       "      <td>-0.870501</td>\n",
       "      <td>0.896570</td>\n",
       "      <td>-1.265312</td>\n",
       "      <td>-0.064263</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.904038</td>\n",
       "      <td>-0.783829</td>\n",
       "      <td>-0.612597</td>\n",
       "      <td>0.058214</td>\n",
       "      <td>-0.657690</td>\n",
       "      <td>0.468953</td>\n",
       "      <td>0.783177</td>\n",
       "      <td>0.528017</td>\n",
       "      <td>0.952097</td>\n",
       "      <td>0.158702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>143_10001143_1602_ch1</td>\n",
       "      <td>143_1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.086748</td>\n",
       "      <td>-0.659327</td>\n",
       "      <td>0.257502</td>\n",
       "      <td>-2.165888</td>\n",
       "      <td>0.320460</td>\n",
       "      <td>...</td>\n",
       "      <td>1.509905</td>\n",
       "      <td>-0.202921</td>\n",
       "      <td>0.492288</td>\n",
       "      <td>-0.447707</td>\n",
       "      <td>-1.585893</td>\n",
       "      <td>0.333516</td>\n",
       "      <td>1.199462</td>\n",
       "      <td>-0.947476</td>\n",
       "      <td>0.438035</td>\n",
       "      <td>0.547364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>143_10001143_1603_ch1</td>\n",
       "      <td>143_1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.879164</td>\n",
       "      <td>-0.906804</td>\n",
       "      <td>1.111726</td>\n",
       "      <td>-1.359166</td>\n",
       "      <td>0.459460</td>\n",
       "      <td>...</td>\n",
       "      <td>0.705248</td>\n",
       "      <td>0.331388</td>\n",
       "      <td>-0.311380</td>\n",
       "      <td>-0.136060</td>\n",
       "      <td>-0.176556</td>\n",
       "      <td>0.279824</td>\n",
       "      <td>-0.674588</td>\n",
       "      <td>-0.243078</td>\n",
       "      <td>1.580138</td>\n",
       "      <td>-1.699354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>143_10001143_1604_ch1</td>\n",
       "      <td>143_1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.746049</td>\n",
       "      <td>-0.896979</td>\n",
       "      <td>-0.491833</td>\n",
       "      <td>-2.354342</td>\n",
       "      <td>-0.458636</td>\n",
       "      <td>...</td>\n",
       "      <td>0.609031</td>\n",
       "      <td>0.720898</td>\n",
       "      <td>0.438264</td>\n",
       "      <td>0.552379</td>\n",
       "      <td>-0.480780</td>\n",
       "      <td>0.584550</td>\n",
       "      <td>0.638610</td>\n",
       "      <td>-0.626269</td>\n",
       "      <td>-1.276253</td>\n",
       "      <td>1.280746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>143_10001143_1605_ch1</td>\n",
       "      <td>143_1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.586725</td>\n",
       "      <td>-0.337968</td>\n",
       "      <td>-0.969250</td>\n",
       "      <td>-2.898370</td>\n",
       "      <td>-0.461360</td>\n",
       "      <td>...</td>\n",
       "      <td>0.322893</td>\n",
       "      <td>-0.549477</td>\n",
       "      <td>0.187279</td>\n",
       "      <td>-2.082130</td>\n",
       "      <td>-0.328702</td>\n",
       "      <td>0.237509</td>\n",
       "      <td>1.280434</td>\n",
       "      <td>0.309391</td>\n",
       "      <td>-1.045902</td>\n",
       "      <td>0.465395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>143_10001143_1606_ch1</td>\n",
       "      <td>143_1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.862793</td>\n",
       "      <td>-0.654877</td>\n",
       "      <td>0.039897</td>\n",
       "      <td>-1.887471</td>\n",
       "      <td>0.387911</td>\n",
       "      <td>...</td>\n",
       "      <td>0.688661</td>\n",
       "      <td>-0.432575</td>\n",
       "      <td>-0.150427</td>\n",
       "      <td>1.853189</td>\n",
       "      <td>0.453691</td>\n",
       "      <td>0.509668</td>\n",
       "      <td>0.144804</td>\n",
       "      <td>0.040649</td>\n",
       "      <td>-0.045122</td>\n",
       "      <td>0.112616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>143_10001143_1753_ch1</td>\n",
       "      <td>143_1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.669157</td>\n",
       "      <td>-0.680628</td>\n",
       "      <td>0.987023</td>\n",
       "      <td>-1.423925</td>\n",
       "      <td>0.656671</td>\n",
       "      <td>...</td>\n",
       "      <td>0.643809</td>\n",
       "      <td>0.081032</td>\n",
       "      <td>0.283785</td>\n",
       "      <td>0.210066</td>\n",
       "      <td>1.903762</td>\n",
       "      <td>-0.122358</td>\n",
       "      <td>-1.498254</td>\n",
       "      <td>-0.941137</td>\n",
       "      <td>0.295074</td>\n",
       "      <td>-2.267138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>143_10001143_1758_ch1</td>\n",
       "      <td>143_1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.049254</td>\n",
       "      <td>-0.793665</td>\n",
       "      <td>0.673366</td>\n",
       "      <td>-1.010624</td>\n",
       "      <td>-0.100845</td>\n",
       "      <td>...</td>\n",
       "      <td>0.880774</td>\n",
       "      <td>-1.620139</td>\n",
       "      <td>0.508040</td>\n",
       "      <td>0.427942</td>\n",
       "      <td>-1.600556</td>\n",
       "      <td>0.246210</td>\n",
       "      <td>-0.795657</td>\n",
       "      <td>-0.036287</td>\n",
       "      <td>1.626201</td>\n",
       "      <td>-3.967272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>143_10001143_1766_ch1</td>\n",
       "      <td>143_1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.377056</td>\n",
       "      <td>-0.730560</td>\n",
       "      <td>1.349984</td>\n",
       "      <td>-0.673848</td>\n",
       "      <td>0.797349</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.117053</td>\n",
       "      <td>-1.211174</td>\n",
       "      <td>-0.285472</td>\n",
       "      <td>-0.839388</td>\n",
       "      <td>0.789911</td>\n",
       "      <td>0.288444</td>\n",
       "      <td>0.704197</td>\n",
       "      <td>1.187611</td>\n",
       "      <td>-0.508897</td>\n",
       "      <td>-0.913957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>143_10001143_1771_ch1</td>\n",
       "      <td>143_1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.246934</td>\n",
       "      <td>-1.060993</td>\n",
       "      <td>0.986592</td>\n",
       "      <td>-1.422845</td>\n",
       "      <td>-0.173418</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.059178</td>\n",
       "      <td>0.925295</td>\n",
       "      <td>0.405071</td>\n",
       "      <td>-0.795963</td>\n",
       "      <td>0.151569</td>\n",
       "      <td>0.965080</td>\n",
       "      <td>-0.090555</td>\n",
       "      <td>-0.161996</td>\n",
       "      <td>0.418260</td>\n",
       "      <td>-0.584478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>143_10001143_1772_ch1</td>\n",
       "      <td>143_1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.830229</td>\n",
       "      <td>-0.848587</td>\n",
       "      <td>1.469294</td>\n",
       "      <td>-1.216774</td>\n",
       "      <td>0.602123</td>\n",
       "      <td>...</td>\n",
       "      <td>2.089509</td>\n",
       "      <td>0.506745</td>\n",
       "      <td>-0.889318</td>\n",
       "      <td>-0.326227</td>\n",
       "      <td>0.920312</td>\n",
       "      <td>0.187032</td>\n",
       "      <td>-1.365379</td>\n",
       "      <td>0.440010</td>\n",
       "      <td>1.876038</td>\n",
       "      <td>0.438103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>143_10001143_1773_ch1</td>\n",
       "      <td>143_1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.764578</td>\n",
       "      <td>-0.598098</td>\n",
       "      <td>1.049717</td>\n",
       "      <td>-0.960591</td>\n",
       "      <td>0.847895</td>\n",
       "      <td>...</td>\n",
       "      <td>0.366753</td>\n",
       "      <td>-0.642463</td>\n",
       "      <td>1.235272</td>\n",
       "      <td>-0.943891</td>\n",
       "      <td>3.092746</td>\n",
       "      <td>-0.567121</td>\n",
       "      <td>-1.521625</td>\n",
       "      <td>0.853881</td>\n",
       "      <td>-0.359619</td>\n",
       "      <td>-1.561343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>143_10001143_1869_ch1</td>\n",
       "      <td>143_1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.477471</td>\n",
       "      <td>-0.803485</td>\n",
       "      <td>1.531498</td>\n",
       "      <td>-0.601012</td>\n",
       "      <td>0.967423</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.068994</td>\n",
       "      <td>-0.976308</td>\n",
       "      <td>0.852991</td>\n",
       "      <td>0.976043</td>\n",
       "      <td>-1.173018</td>\n",
       "      <td>-1.164614</td>\n",
       "      <td>1.402002</td>\n",
       "      <td>0.083167</td>\n",
       "      <td>2.412549</td>\n",
       "      <td>0.305316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>143_10001143_1870_ch1</td>\n",
       "      <td>143_1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.497390</td>\n",
       "      <td>-0.812797</td>\n",
       "      <td>1.006634</td>\n",
       "      <td>-1.199287</td>\n",
       "      <td>0.578207</td>\n",
       "      <td>...</td>\n",
       "      <td>0.918222</td>\n",
       "      <td>0.917191</td>\n",
       "      <td>-0.836664</td>\n",
       "      <td>0.283661</td>\n",
       "      <td>0.056439</td>\n",
       "      <td>1.071746</td>\n",
       "      <td>0.246297</td>\n",
       "      <td>0.351041</td>\n",
       "      <td>-0.839595</td>\n",
       "      <td>0.834650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>143_10001143_1871_ch1</td>\n",
       "      <td>143_1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.829530</td>\n",
       "      <td>-0.774017</td>\n",
       "      <td>1.379009</td>\n",
       "      <td>-0.793920</td>\n",
       "      <td>0.498256</td>\n",
       "      <td>...</td>\n",
       "      <td>0.126982</td>\n",
       "      <td>0.291561</td>\n",
       "      <td>-1.575945</td>\n",
       "      <td>0.782344</td>\n",
       "      <td>0.942874</td>\n",
       "      <td>1.157992</td>\n",
       "      <td>-0.466651</td>\n",
       "      <td>-0.227680</td>\n",
       "      <td>-0.666037</td>\n",
       "      <td>0.556310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>143_10001143_1_ch1</td>\n",
       "      <td>143_1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.967348</td>\n",
       "      <td>-1.008688</td>\n",
       "      <td>1.005477</td>\n",
       "      <td>-1.727103</td>\n",
       "      <td>0.786502</td>\n",
       "      <td>...</td>\n",
       "      <td>0.226649</td>\n",
       "      <td>-0.734023</td>\n",
       "      <td>-0.107121</td>\n",
       "      <td>-0.021536</td>\n",
       "      <td>-1.392008</td>\n",
       "      <td>0.152094</td>\n",
       "      <td>-0.007981</td>\n",
       "      <td>0.930795</td>\n",
       "      <td>0.045467</td>\n",
       "      <td>-0.819174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>143_10001143_216_ch1</td>\n",
       "      <td>143_1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.329952</td>\n",
       "      <td>-0.492884</td>\n",
       "      <td>0.472935</td>\n",
       "      <td>-2.288254</td>\n",
       "      <td>0.977964</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.507166</td>\n",
       "      <td>-0.703086</td>\n",
       "      <td>-0.144407</td>\n",
       "      <td>1.011852</td>\n",
       "      <td>-1.806159</td>\n",
       "      <td>0.858029</td>\n",
       "      <td>1.840403</td>\n",
       "      <td>-0.265116</td>\n",
       "      <td>-1.386902</td>\n",
       "      <td>-0.277597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>143_10001143_2175_ch1</td>\n",
       "      <td>143_1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.254757</td>\n",
       "      <td>-0.588089</td>\n",
       "      <td>0.216287</td>\n",
       "      <td>-1.977390</td>\n",
       "      <td>0.678083</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.355296</td>\n",
       "      <td>-1.756499</td>\n",
       "      <td>1.736595</td>\n",
       "      <td>-0.175708</td>\n",
       "      <td>1.274580</td>\n",
       "      <td>-0.864184</td>\n",
       "      <td>0.406857</td>\n",
       "      <td>0.170143</td>\n",
       "      <td>-0.412133</td>\n",
       "      <td>-0.125966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>143_10001143_217_ch1</td>\n",
       "      <td>143_1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.531983</td>\n",
       "      <td>-0.087899</td>\n",
       "      <td>-1.527658</td>\n",
       "      <td>-2.528104</td>\n",
       "      <td>0.446994</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.294816</td>\n",
       "      <td>0.060066</td>\n",
       "      <td>0.079945</td>\n",
       "      <td>-0.080741</td>\n",
       "      <td>-2.463592</td>\n",
       "      <td>0.786150</td>\n",
       "      <td>1.475682</td>\n",
       "      <td>0.236868</td>\n",
       "      <td>0.049370</td>\n",
       "      <td>1.805464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>143_10001143_219_ch1</td>\n",
       "      <td>143_1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.606818</td>\n",
       "      <td>-1.083599</td>\n",
       "      <td>1.617952</td>\n",
       "      <td>-0.697788</td>\n",
       "      <td>-0.444704</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.106509</td>\n",
       "      <td>1.066072</td>\n",
       "      <td>0.507474</td>\n",
       "      <td>0.006458</td>\n",
       "      <td>0.316867</td>\n",
       "      <td>1.891398</td>\n",
       "      <td>1.753680</td>\n",
       "      <td>0.702816</td>\n",
       "      <td>-0.649200</td>\n",
       "      <td>-0.017082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>143_10001143_220_ch1</td>\n",
       "      <td>143_1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.624768</td>\n",
       "      <td>-1.260921</td>\n",
       "      <td>1.720106</td>\n",
       "      <td>-0.715174</td>\n",
       "      <td>1.012929</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.094074</td>\n",
       "      <td>-0.296034</td>\n",
       "      <td>0.456789</td>\n",
       "      <td>-0.391078</td>\n",
       "      <td>-2.300726</td>\n",
       "      <td>1.532758</td>\n",
       "      <td>-1.082500</td>\n",
       "      <td>-1.473471</td>\n",
       "      <td>-0.341343</td>\n",
       "      <td>-0.015172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>143_10001143_2225_ch1</td>\n",
       "      <td>143_1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.184358</td>\n",
       "      <td>-0.611788</td>\n",
       "      <td>0.405108</td>\n",
       "      <td>-1.647106</td>\n",
       "      <td>-0.050389</td>\n",
       "      <td>...</td>\n",
       "      <td>1.373300</td>\n",
       "      <td>1.031482</td>\n",
       "      <td>0.960797</td>\n",
       "      <td>0.362437</td>\n",
       "      <td>-0.283865</td>\n",
       "      <td>-1.058123</td>\n",
       "      <td>-0.581387</td>\n",
       "      <td>-0.574496</td>\n",
       "      <td>0.260336</td>\n",
       "      <td>-0.392679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2140</th>\n",
       "      <td>160_20002160_40_ch1</td>\n",
       "      <td>160_2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.580273</td>\n",
       "      <td>-1.031550</td>\n",
       "      <td>0.665466</td>\n",
       "      <td>0.883108</td>\n",
       "      <td>0.564057</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.627788</td>\n",
       "      <td>-1.692672</td>\n",
       "      <td>-0.134109</td>\n",
       "      <td>-0.312507</td>\n",
       "      <td>1.198725</td>\n",
       "      <td>-1.114402</td>\n",
       "      <td>0.656526</td>\n",
       "      <td>1.833696</td>\n",
       "      <td>0.192125</td>\n",
       "      <td>0.819336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2141</th>\n",
       "      <td>160_20002160_452_ch1</td>\n",
       "      <td>160_2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.127073</td>\n",
       "      <td>-0.547758</td>\n",
       "      <td>0.108654</td>\n",
       "      <td>0.877373</td>\n",
       "      <td>-0.038110</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.580326</td>\n",
       "      <td>2.555126</td>\n",
       "      <td>-0.109799</td>\n",
       "      <td>-1.239605</td>\n",
       "      <td>-0.313155</td>\n",
       "      <td>0.907743</td>\n",
       "      <td>-0.647262</td>\n",
       "      <td>-0.182566</td>\n",
       "      <td>-0.932861</td>\n",
       "      <td>0.437980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2142</th>\n",
       "      <td>160_20002160_476_ch1</td>\n",
       "      <td>160_2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1.211100</td>\n",
       "      <td>-0.460814</td>\n",
       "      <td>0.347023</td>\n",
       "      <td>0.398316</td>\n",
       "      <td>-0.507045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555948</td>\n",
       "      <td>-1.308376</td>\n",
       "      <td>0.308844</td>\n",
       "      <td>-0.105314</td>\n",
       "      <td>-1.577031</td>\n",
       "      <td>-0.054685</td>\n",
       "      <td>0.896320</td>\n",
       "      <td>-0.252131</td>\n",
       "      <td>0.143151</td>\n",
       "      <td>-0.226842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2143</th>\n",
       "      <td>160_20002160_4_ch1</td>\n",
       "      <td>160_2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.326029</td>\n",
       "      <td>-1.015287</td>\n",
       "      <td>0.613450</td>\n",
       "      <td>-0.165080</td>\n",
       "      <td>-0.255155</td>\n",
       "      <td>...</td>\n",
       "      <td>0.858734</td>\n",
       "      <td>-0.059058</td>\n",
       "      <td>-1.635842</td>\n",
       "      <td>-0.729582</td>\n",
       "      <td>0.364365</td>\n",
       "      <td>1.305921</td>\n",
       "      <td>-1.335699</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>-1.326234</td>\n",
       "      <td>-0.437820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2144</th>\n",
       "      <td>160_20002160_56_ch1</td>\n",
       "      <td>160_2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.003813</td>\n",
       "      <td>-0.758796</td>\n",
       "      <td>0.426522</td>\n",
       "      <td>-0.539192</td>\n",
       "      <td>-0.261853</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.173583</td>\n",
       "      <td>0.394818</td>\n",
       "      <td>1.196750</td>\n",
       "      <td>-0.756522</td>\n",
       "      <td>-1.628930</td>\n",
       "      <td>-0.282916</td>\n",
       "      <td>-0.297052</td>\n",
       "      <td>0.180510</td>\n",
       "      <td>-0.292901</td>\n",
       "      <td>0.210189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2145</th>\n",
       "      <td>160_20002160_57_ch1</td>\n",
       "      <td>160_2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.814649</td>\n",
       "      <td>-0.787864</td>\n",
       "      <td>0.358707</td>\n",
       "      <td>0.284442</td>\n",
       "      <td>-0.024010</td>\n",
       "      <td>...</td>\n",
       "      <td>1.185343</td>\n",
       "      <td>0.140252</td>\n",
       "      <td>-0.522696</td>\n",
       "      <td>-0.026307</td>\n",
       "      <td>-0.363279</td>\n",
       "      <td>-0.243408</td>\n",
       "      <td>-0.733637</td>\n",
       "      <td>-0.517996</td>\n",
       "      <td>0.991660</td>\n",
       "      <td>0.322341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2146</th>\n",
       "      <td>160_20002160_58_ch1</td>\n",
       "      <td>160_2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.267237</td>\n",
       "      <td>-0.489571</td>\n",
       "      <td>-0.012346</td>\n",
       "      <td>-0.312471</td>\n",
       "      <td>-0.531811</td>\n",
       "      <td>...</td>\n",
       "      <td>0.734925</td>\n",
       "      <td>-0.491161</td>\n",
       "      <td>1.175151</td>\n",
       "      <td>-1.604911</td>\n",
       "      <td>-0.176901</td>\n",
       "      <td>-0.745546</td>\n",
       "      <td>1.627408</td>\n",
       "      <td>-0.233297</td>\n",
       "      <td>0.789341</td>\n",
       "      <td>-0.076185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2147</th>\n",
       "      <td>160_20002160_5_ch1</td>\n",
       "      <td>160_2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.134893</td>\n",
       "      <td>-0.387783</td>\n",
       "      <td>0.395950</td>\n",
       "      <td>0.237082</td>\n",
       "      <td>-0.018130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.981907</td>\n",
       "      <td>0.904745</td>\n",
       "      <td>0.779069</td>\n",
       "      <td>0.970563</td>\n",
       "      <td>0.601514</td>\n",
       "      <td>0.008572</td>\n",
       "      <td>0.906337</td>\n",
       "      <td>-0.557621</td>\n",
       "      <td>-0.430310</td>\n",
       "      <td>0.745636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2148</th>\n",
       "      <td>160_20002160_630_ch1</td>\n",
       "      <td>160_2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.301902</td>\n",
       "      <td>0.419892</td>\n",
       "      <td>-2.646945</td>\n",
       "      <td>0.340909</td>\n",
       "      <td>0.461617</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.475276</td>\n",
       "      <td>-1.552198</td>\n",
       "      <td>0.159359</td>\n",
       "      <td>2.206533</td>\n",
       "      <td>-0.113028</td>\n",
       "      <td>0.077018</td>\n",
       "      <td>-0.275131</td>\n",
       "      <td>0.605041</td>\n",
       "      <td>1.122494</td>\n",
       "      <td>-0.330695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2149</th>\n",
       "      <td>160_20002160_654_ch1</td>\n",
       "      <td>160_2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.408308</td>\n",
       "      <td>0.013742</td>\n",
       "      <td>-1.523983</td>\n",
       "      <td>-0.623424</td>\n",
       "      <td>-0.304642</td>\n",
       "      <td>...</td>\n",
       "      <td>0.690363</td>\n",
       "      <td>-0.412769</td>\n",
       "      <td>-0.094418</td>\n",
       "      <td>-0.276782</td>\n",
       "      <td>0.216772</td>\n",
       "      <td>0.070612</td>\n",
       "      <td>-0.562685</td>\n",
       "      <td>0.533959</td>\n",
       "      <td>-0.546800</td>\n",
       "      <td>0.119710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150</th>\n",
       "      <td>160_20002160_659_ch1</td>\n",
       "      <td>160_2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1.111716</td>\n",
       "      <td>-1.033821</td>\n",
       "      <td>0.348565</td>\n",
       "      <td>1.230576</td>\n",
       "      <td>0.106976</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.158768</td>\n",
       "      <td>-1.354152</td>\n",
       "      <td>0.341270</td>\n",
       "      <td>-0.296130</td>\n",
       "      <td>0.173496</td>\n",
       "      <td>-0.862393</td>\n",
       "      <td>-1.533995</td>\n",
       "      <td>0.385015</td>\n",
       "      <td>1.061645</td>\n",
       "      <td>-0.413411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2151</th>\n",
       "      <td>160_20002160_683_ch1</td>\n",
       "      <td>160_2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.345824</td>\n",
       "      <td>-0.564899</td>\n",
       "      <td>0.794396</td>\n",
       "      <td>-0.095711</td>\n",
       "      <td>-0.885366</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.018097</td>\n",
       "      <td>1.272374</td>\n",
       "      <td>-0.519678</td>\n",
       "      <td>-0.828041</td>\n",
       "      <td>0.573271</td>\n",
       "      <td>1.358465</td>\n",
       "      <td>-0.686744</td>\n",
       "      <td>-0.884768</td>\n",
       "      <td>0.792909</td>\n",
       "      <td>0.052860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2152</th>\n",
       "      <td>160_20002160_68_ch1</td>\n",
       "      <td>160_2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.221337</td>\n",
       "      <td>-0.920697</td>\n",
       "      <td>0.369987</td>\n",
       "      <td>-0.007290</td>\n",
       "      <td>0.275131</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.191463</td>\n",
       "      <td>0.937813</td>\n",
       "      <td>0.290205</td>\n",
       "      <td>-0.370203</td>\n",
       "      <td>-0.411746</td>\n",
       "      <td>-1.200070</td>\n",
       "      <td>1.558526</td>\n",
       "      <td>-0.253793</td>\n",
       "      <td>1.200992</td>\n",
       "      <td>0.497301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2153</th>\n",
       "      <td>160_20002160_69_ch1</td>\n",
       "      <td>160_2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.847478</td>\n",
       "      <td>-0.607212</td>\n",
       "      <td>0.660623</td>\n",
       "      <td>0.103179</td>\n",
       "      <td>0.011619</td>\n",
       "      <td>...</td>\n",
       "      <td>0.309856</td>\n",
       "      <td>0.298147</td>\n",
       "      <td>0.272378</td>\n",
       "      <td>0.493684</td>\n",
       "      <td>0.467458</td>\n",
       "      <td>-0.546313</td>\n",
       "      <td>2.608574</td>\n",
       "      <td>-0.632433</td>\n",
       "      <td>-0.155765</td>\n",
       "      <td>1.368509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2154</th>\n",
       "      <td>160_20002160_6_ch1</td>\n",
       "      <td>160_2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.795505</td>\n",
       "      <td>-0.425568</td>\n",
       "      <td>-0.062756</td>\n",
       "      <td>0.954737</td>\n",
       "      <td>0.003568</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.010457</td>\n",
       "      <td>-1.608230</td>\n",
       "      <td>-0.776497</td>\n",
       "      <td>1.100839</td>\n",
       "      <td>-0.060823</td>\n",
       "      <td>0.371421</td>\n",
       "      <td>0.658277</td>\n",
       "      <td>1.118108</td>\n",
       "      <td>0.102180</td>\n",
       "      <td>2.087003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2155</th>\n",
       "      <td>160_20002160_70_ch1</td>\n",
       "      <td>160_2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.029444</td>\n",
       "      <td>-0.290953</td>\n",
       "      <td>0.313923</td>\n",
       "      <td>-0.196137</td>\n",
       "      <td>-0.139479</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.620991</td>\n",
       "      <td>-1.045799</td>\n",
       "      <td>0.329202</td>\n",
       "      <td>0.500131</td>\n",
       "      <td>2.004157</td>\n",
       "      <td>-0.669703</td>\n",
       "      <td>-1.128691</td>\n",
       "      <td>0.024542</td>\n",
       "      <td>-0.481156</td>\n",
       "      <td>1.131981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2156</th>\n",
       "      <td>160_20002160_71_ch1</td>\n",
       "      <td>160_2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.678849</td>\n",
       "      <td>-0.548184</td>\n",
       "      <td>-0.272191</td>\n",
       "      <td>0.038367</td>\n",
       "      <td>-0.415148</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.179679</td>\n",
       "      <td>1.171526</td>\n",
       "      <td>-1.369484</td>\n",
       "      <td>-0.360422</td>\n",
       "      <td>0.117195</td>\n",
       "      <td>0.222658</td>\n",
       "      <td>0.204221</td>\n",
       "      <td>0.662556</td>\n",
       "      <td>-0.200390</td>\n",
       "      <td>-0.253936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2157</th>\n",
       "      <td>160_20002160_72_ch1</td>\n",
       "      <td>160_2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.272095</td>\n",
       "      <td>-0.659133</td>\n",
       "      <td>-0.030406</td>\n",
       "      <td>-0.185416</td>\n",
       "      <td>-0.305561</td>\n",
       "      <td>...</td>\n",
       "      <td>0.218312</td>\n",
       "      <td>-0.122961</td>\n",
       "      <td>0.960783</td>\n",
       "      <td>-0.210329</td>\n",
       "      <td>0.566338</td>\n",
       "      <td>-1.607740</td>\n",
       "      <td>0.397980</td>\n",
       "      <td>0.091519</td>\n",
       "      <td>0.785763</td>\n",
       "      <td>-0.363546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158</th>\n",
       "      <td>160_20002160_73_ch1</td>\n",
       "      <td>160_2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.450915</td>\n",
       "      <td>-0.571424</td>\n",
       "      <td>0.664927</td>\n",
       "      <td>-0.363237</td>\n",
       "      <td>0.385481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.518619</td>\n",
       "      <td>0.745547</td>\n",
       "      <td>1.149459</td>\n",
       "      <td>-0.676299</td>\n",
       "      <td>0.771929</td>\n",
       "      <td>0.260440</td>\n",
       "      <td>1.064718</td>\n",
       "      <td>-0.258190</td>\n",
       "      <td>-1.649790</td>\n",
       "      <td>1.650524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2159</th>\n",
       "      <td>160_20002160_74_ch1</td>\n",
       "      <td>160_2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.643365</td>\n",
       "      <td>-0.342332</td>\n",
       "      <td>0.279351</td>\n",
       "      <td>-0.088643</td>\n",
       "      <td>-0.713959</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.492494</td>\n",
       "      <td>-1.829173</td>\n",
       "      <td>1.729330</td>\n",
       "      <td>-0.486163</td>\n",
       "      <td>0.775802</td>\n",
       "      <td>0.619463</td>\n",
       "      <td>1.467574</td>\n",
       "      <td>-1.334137</td>\n",
       "      <td>-0.064082</td>\n",
       "      <td>1.900484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2160</th>\n",
       "      <td>160_20002160_755_ch1</td>\n",
       "      <td>160_2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.540232</td>\n",
       "      <td>-0.467479</td>\n",
       "      <td>0.896142</td>\n",
       "      <td>0.495247</td>\n",
       "      <td>-0.578866</td>\n",
       "      <td>...</td>\n",
       "      <td>0.165738</td>\n",
       "      <td>0.505194</td>\n",
       "      <td>-1.271706</td>\n",
       "      <td>-0.233622</td>\n",
       "      <td>0.232578</td>\n",
       "      <td>-0.240881</td>\n",
       "      <td>0.806048</td>\n",
       "      <td>-0.608192</td>\n",
       "      <td>0.729197</td>\n",
       "      <td>-0.403015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2161</th>\n",
       "      <td>160_20002160_756_ch1</td>\n",
       "      <td>160_2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.503218</td>\n",
       "      <td>-0.361195</td>\n",
       "      <td>0.407393</td>\n",
       "      <td>-0.014351</td>\n",
       "      <td>-0.058014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.233128</td>\n",
       "      <td>-1.048495</td>\n",
       "      <td>-0.312241</td>\n",
       "      <td>-0.090365</td>\n",
       "      <td>1.967659</td>\n",
       "      <td>-0.191724</td>\n",
       "      <td>-0.100377</td>\n",
       "      <td>0.845229</td>\n",
       "      <td>0.518966</td>\n",
       "      <td>0.183473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2162</th>\n",
       "      <td>160_20002160_757_ch1</td>\n",
       "      <td>160_2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.876457</td>\n",
       "      <td>-0.769740</td>\n",
       "      <td>1.453737</td>\n",
       "      <td>0.599947</td>\n",
       "      <td>-0.179622</td>\n",
       "      <td>...</td>\n",
       "      <td>1.546808</td>\n",
       "      <td>0.491209</td>\n",
       "      <td>-1.571067</td>\n",
       "      <td>-0.233093</td>\n",
       "      <td>-0.319234</td>\n",
       "      <td>-1.225319</td>\n",
       "      <td>-0.531166</td>\n",
       "      <td>-0.870199</td>\n",
       "      <td>1.261102</td>\n",
       "      <td>1.573236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2163</th>\n",
       "      <td>160_20002160_758_ch1</td>\n",
       "      <td>160_2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.167690</td>\n",
       "      <td>-1.087387</td>\n",
       "      <td>1.165254</td>\n",
       "      <td>0.407514</td>\n",
       "      <td>0.031691</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.762699</td>\n",
       "      <td>-0.393018</td>\n",
       "      <td>-3.044596</td>\n",
       "      <td>-0.017744</td>\n",
       "      <td>-0.326368</td>\n",
       "      <td>-0.645144</td>\n",
       "      <td>0.774340</td>\n",
       "      <td>-0.719842</td>\n",
       "      <td>1.027182</td>\n",
       "      <td>-0.153171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2164</th>\n",
       "      <td>160_20002160_75_ch1</td>\n",
       "      <td>160_2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.136864</td>\n",
       "      <td>-0.693460</td>\n",
       "      <td>0.534947</td>\n",
       "      <td>0.343101</td>\n",
       "      <td>-0.271073</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050476</td>\n",
       "      <td>-0.048363</td>\n",
       "      <td>0.857611</td>\n",
       "      <td>0.084989</td>\n",
       "      <td>0.514933</td>\n",
       "      <td>0.107351</td>\n",
       "      <td>1.172797</td>\n",
       "      <td>0.346443</td>\n",
       "      <td>1.684682</td>\n",
       "      <td>-0.891121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2165</th>\n",
       "      <td>160_20002160_76_ch1</td>\n",
       "      <td>160_2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.665772</td>\n",
       "      <td>-1.221192</td>\n",
       "      <td>-0.037430</td>\n",
       "      <td>1.085606</td>\n",
       "      <td>0.219417</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.935748</td>\n",
       "      <td>0.438242</td>\n",
       "      <td>-0.711602</td>\n",
       "      <td>0.908138</td>\n",
       "      <td>-1.687777</td>\n",
       "      <td>-0.721695</td>\n",
       "      <td>0.448478</td>\n",
       "      <td>0.548281</td>\n",
       "      <td>0.277385</td>\n",
       "      <td>-0.809920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2166</th>\n",
       "      <td>160_20002160_7_ch1</td>\n",
       "      <td>160_2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.161996</td>\n",
       "      <td>-0.474195</td>\n",
       "      <td>0.129074</td>\n",
       "      <td>0.951555</td>\n",
       "      <td>-0.273831</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.673105</td>\n",
       "      <td>-0.018660</td>\n",
       "      <td>0.082834</td>\n",
       "      <td>1.377634</td>\n",
       "      <td>0.088779</td>\n",
       "      <td>0.524701</td>\n",
       "      <td>-0.539860</td>\n",
       "      <td>0.301450</td>\n",
       "      <td>-0.245628</td>\n",
       "      <td>-0.771783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2167</th>\n",
       "      <td>160_20002160_815_ch1</td>\n",
       "      <td>160_2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.484400</td>\n",
       "      <td>-0.429804</td>\n",
       "      <td>0.925534</td>\n",
       "      <td>0.670530</td>\n",
       "      <td>-0.342585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080191</td>\n",
       "      <td>0.140669</td>\n",
       "      <td>-2.010542</td>\n",
       "      <td>-0.782719</td>\n",
       "      <td>1.206946</td>\n",
       "      <td>0.014356</td>\n",
       "      <td>1.263567</td>\n",
       "      <td>0.222758</td>\n",
       "      <td>0.822307</td>\n",
       "      <td>-0.692752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2168</th>\n",
       "      <td>160_20002160_8_ch1</td>\n",
       "      <td>160_2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2.089146</td>\n",
       "      <td>-0.296144</td>\n",
       "      <td>0.259379</td>\n",
       "      <td>1.050148</td>\n",
       "      <td>-0.634846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.647596</td>\n",
       "      <td>-1.174770</td>\n",
       "      <td>-0.029580</td>\n",
       "      <td>-0.774872</td>\n",
       "      <td>-1.400781</td>\n",
       "      <td>-1.683537</td>\n",
       "      <td>-0.668288</td>\n",
       "      <td>-0.500911</td>\n",
       "      <td>1.166515</td>\n",
       "      <td>-2.042598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2169</th>\n",
       "      <td>160_20002160_9_ch1</td>\n",
       "      <td>160_2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.884954</td>\n",
       "      <td>-0.505655</td>\n",
       "      <td>-0.933143</td>\n",
       "      <td>1.300051</td>\n",
       "      <td>1.199140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.245675</td>\n",
       "      <td>0.299113</td>\n",
       "      <td>-0.260392</td>\n",
       "      <td>-0.950292</td>\n",
       "      <td>0.847789</td>\n",
       "      <td>0.745867</td>\n",
       "      <td>0.345427</td>\n",
       "      <td>0.019123</td>\n",
       "      <td>-1.275937</td>\n",
       "      <td>0.668629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>280 rows × 605 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Utterance Speaker Gender  Age AgeGroup     ivec1     ivec2  \\\n",
       "0      143_10001143_100_ch1   143_1      0    9        0 -0.418398 -0.938296   \n",
       "1       143_10001143_10_ch1   143_1      0    9        0 -0.633367 -0.525246   \n",
       "2      143_10001143_129_ch1   143_1      0    9        0 -1.066284  0.357609   \n",
       "3     143_10001143_1432_ch1   143_1      0    9        0 -0.592274 -0.974527   \n",
       "4     143_10001143_1433_ch1   143_1      0    9        0 -1.050412 -0.750296   \n",
       "5     143_10001143_1434_ch1   143_1      0    9        0 -0.627047 -0.383151   \n",
       "6     143_10001143_1435_ch1   143_1      0    9        0 -1.078822 -0.456977   \n",
       "7      143_10001143_157_ch1   143_1      0    9        0 -0.300843 -0.825967   \n",
       "8      143_10001143_158_ch1   143_1      0    9        0 -1.177312 -0.870501   \n",
       "9     143_10001143_1602_ch1   143_1      0    9        0 -1.086748 -0.659327   \n",
       "10    143_10001143_1603_ch1   143_1      0    9        0 -1.879164 -0.906804   \n",
       "11    143_10001143_1604_ch1   143_1      0    9        0 -0.746049 -0.896979   \n",
       "12    143_10001143_1605_ch1   143_1      0    9        0 -1.586725 -0.337968   \n",
       "13    143_10001143_1606_ch1   143_1      0    9        0 -0.862793 -0.654877   \n",
       "14    143_10001143_1753_ch1   143_1      0    9        0 -1.669157 -0.680628   \n",
       "15    143_10001143_1758_ch1   143_1      0    9        0  0.049254 -0.793665   \n",
       "16    143_10001143_1766_ch1   143_1      0    9        0 -0.377056 -0.730560   \n",
       "17    143_10001143_1771_ch1   143_1      0    9        0 -0.246934 -1.060993   \n",
       "18    143_10001143_1772_ch1   143_1      0    9        0 -0.830229 -0.848587   \n",
       "19    143_10001143_1773_ch1   143_1      0    9        0 -0.764578 -0.598098   \n",
       "20    143_10001143_1869_ch1   143_1      0    9        0 -0.477471 -0.803485   \n",
       "21    143_10001143_1870_ch1   143_1      0    9        0 -1.497390 -0.812797   \n",
       "22    143_10001143_1871_ch1   143_1      0    9        0  0.829530 -0.774017   \n",
       "23       143_10001143_1_ch1   143_1      0    9        0 -1.967348 -1.008688   \n",
       "24     143_10001143_216_ch1   143_1      0    9        0  0.329952 -0.492884   \n",
       "25    143_10001143_2175_ch1   143_1      0    9        0 -1.254757 -0.588089   \n",
       "26     143_10001143_217_ch1   143_1      0    9        0 -0.531983 -0.087899   \n",
       "27     143_10001143_219_ch1   143_1      0    9        0 -0.606818 -1.083599   \n",
       "28     143_10001143_220_ch1   143_1      0    9        0  0.624768 -1.260921   \n",
       "29    143_10001143_2225_ch1   143_1      0    9        0 -1.184358 -0.611788   \n",
       "...                     ...     ...    ...  ...      ...       ...       ...   \n",
       "2140    160_20002160_40_ch1   160_2      1    9        0 -0.580273 -1.031550   \n",
       "2141   160_20002160_452_ch1   160_2      1    9        0 -1.127073 -0.547758   \n",
       "2142   160_20002160_476_ch1   160_2      1    9        0  1.211100 -0.460814   \n",
       "2143     160_20002160_4_ch1   160_2      1    9        0  0.326029 -1.015287   \n",
       "2144    160_20002160_56_ch1   160_2      1    9        0 -0.003813 -0.758796   \n",
       "2145    160_20002160_57_ch1   160_2      1    9        0 -0.814649 -0.787864   \n",
       "2146    160_20002160_58_ch1   160_2      1    9        0  0.267237 -0.489571   \n",
       "2147     160_20002160_5_ch1   160_2      1    9        0 -0.134893 -0.387783   \n",
       "2148   160_20002160_630_ch1   160_2      1    9        0 -0.301902  0.419892   \n",
       "2149   160_20002160_654_ch1   160_2      1    9        0 -0.408308  0.013742   \n",
       "2150   160_20002160_659_ch1   160_2      1    9        0  1.111716 -1.033821   \n",
       "2151   160_20002160_683_ch1   160_2      1    9        0  0.345824 -0.564899   \n",
       "2152    160_20002160_68_ch1   160_2      1    9        0  0.221337 -0.920697   \n",
       "2153    160_20002160_69_ch1   160_2      1    9        0 -0.847478 -0.607212   \n",
       "2154     160_20002160_6_ch1   160_2      1    9        0  0.795505 -0.425568   \n",
       "2155    160_20002160_70_ch1   160_2      1    9        0  0.029444 -0.290953   \n",
       "2156    160_20002160_71_ch1   160_2      1    9        0  0.678849 -0.548184   \n",
       "2157    160_20002160_72_ch1   160_2      1    9        0 -0.272095 -0.659133   \n",
       "2158    160_20002160_73_ch1   160_2      1    9        0 -0.450915 -0.571424   \n",
       "2159    160_20002160_74_ch1   160_2      1    9        0  0.643365 -0.342332   \n",
       "2160   160_20002160_755_ch1   160_2      1    9        0  0.540232 -0.467479   \n",
       "2161   160_20002160_756_ch1   160_2      1    9        0  0.503218 -0.361195   \n",
       "2162   160_20002160_757_ch1   160_2      1    9        0 -1.876457 -0.769740   \n",
       "2163   160_20002160_758_ch1   160_2      1    9        0  0.167690 -1.087387   \n",
       "2164    160_20002160_75_ch1   160_2      1    9        0 -1.136864 -0.693460   \n",
       "2165    160_20002160_76_ch1   160_2      1    9        0  0.665772 -1.221192   \n",
       "2166     160_20002160_7_ch1   160_2      1    9        0 -0.161996 -0.474195   \n",
       "2167   160_20002160_815_ch1   160_2      1    9        0 -1.484400 -0.429804   \n",
       "2168     160_20002160_8_ch1   160_2      1    9        0  2.089146 -0.296144   \n",
       "2169     160_20002160_9_ch1   160_2      1    9        0 -0.884954 -0.505655   \n",
       "\n",
       "         ivec3     ivec4     ivec5    ...      ivec591   ivec592   ivec593  \\\n",
       "0     0.669691 -1.463043  0.453503    ...     0.128172  0.934197  0.765379   \n",
       "1     0.424735 -1.210979  1.081761    ...    -0.630734 -0.029553 -0.920571   \n",
       "2    -0.992035 -2.905210  0.573784    ...    -0.894630  0.442736  0.312838   \n",
       "3     0.520979 -0.997169  0.200358    ...     0.966990 -1.037654 -0.314220   \n",
       "4     0.077094 -1.177778  0.081897    ...     0.109936  0.118326  1.261557   \n",
       "5    -0.159873 -1.425629 -0.476528    ...    -0.311192  0.765314  0.593452   \n",
       "6     0.876714 -0.795689  0.072347    ...    -1.540425  0.654016  0.735474   \n",
       "7     0.687509 -1.794576 -0.476436    ...    -0.761839 -0.776795  0.038089   \n",
       "8     0.896570 -1.265312 -0.064263    ...    -0.904038 -0.783829 -0.612597   \n",
       "9     0.257502 -2.165888  0.320460    ...     1.509905 -0.202921  0.492288   \n",
       "10    1.111726 -1.359166  0.459460    ...     0.705248  0.331388 -0.311380   \n",
       "11   -0.491833 -2.354342 -0.458636    ...     0.609031  0.720898  0.438264   \n",
       "12   -0.969250 -2.898370 -0.461360    ...     0.322893 -0.549477  0.187279   \n",
       "13    0.039897 -1.887471  0.387911    ...     0.688661 -0.432575 -0.150427   \n",
       "14    0.987023 -1.423925  0.656671    ...     0.643809  0.081032  0.283785   \n",
       "15    0.673366 -1.010624 -0.100845    ...     0.880774 -1.620139  0.508040   \n",
       "16    1.349984 -0.673848  0.797349    ...    -1.117053 -1.211174 -0.285472   \n",
       "17    0.986592 -1.422845 -0.173418    ...    -1.059178  0.925295  0.405071   \n",
       "18    1.469294 -1.216774  0.602123    ...     2.089509  0.506745 -0.889318   \n",
       "19    1.049717 -0.960591  0.847895    ...     0.366753 -0.642463  1.235272   \n",
       "20    1.531498 -0.601012  0.967423    ...    -1.068994 -0.976308  0.852991   \n",
       "21    1.006634 -1.199287  0.578207    ...     0.918222  0.917191 -0.836664   \n",
       "22    1.379009 -0.793920  0.498256    ...     0.126982  0.291561 -1.575945   \n",
       "23    1.005477 -1.727103  0.786502    ...     0.226649 -0.734023 -0.107121   \n",
       "24    0.472935 -2.288254  0.977964    ...    -0.507166 -0.703086 -0.144407   \n",
       "25    0.216287 -1.977390  0.678083    ...    -0.355296 -1.756499  1.736595   \n",
       "26   -1.527658 -2.528104  0.446994    ...    -1.294816  0.060066  0.079945   \n",
       "27    1.617952 -0.697788 -0.444704    ...    -0.106509  1.066072  0.507474   \n",
       "28    1.720106 -0.715174  1.012929    ...    -1.094074 -0.296034  0.456789   \n",
       "29    0.405108 -1.647106 -0.050389    ...     1.373300  1.031482  0.960797   \n",
       "...        ...       ...       ...    ...          ...       ...       ...   \n",
       "2140  0.665466  0.883108  0.564057    ...    -0.627788 -1.692672 -0.134109   \n",
       "2141  0.108654  0.877373 -0.038110    ...    -0.580326  2.555126 -0.109799   \n",
       "2142  0.347023  0.398316 -0.507045    ...     0.555948 -1.308376  0.308844   \n",
       "2143  0.613450 -0.165080 -0.255155    ...     0.858734 -0.059058 -1.635842   \n",
       "2144  0.426522 -0.539192 -0.261853    ...    -0.173583  0.394818  1.196750   \n",
       "2145  0.358707  0.284442 -0.024010    ...     1.185343  0.140252 -0.522696   \n",
       "2146 -0.012346 -0.312471 -0.531811    ...     0.734925 -0.491161  1.175151   \n",
       "2147  0.395950  0.237082 -0.018130    ...     0.981907  0.904745  0.779069   \n",
       "2148 -2.646945  0.340909  0.461617    ...    -0.475276 -1.552198  0.159359   \n",
       "2149 -1.523983 -0.623424 -0.304642    ...     0.690363 -0.412769 -0.094418   \n",
       "2150  0.348565  1.230576  0.106976    ...    -1.158768 -1.354152  0.341270   \n",
       "2151  0.794396 -0.095711 -0.885366    ...    -2.018097  1.272374 -0.519678   \n",
       "2152  0.369987 -0.007290  0.275131    ...    -0.191463  0.937813  0.290205   \n",
       "2153  0.660623  0.103179  0.011619    ...     0.309856  0.298147  0.272378   \n",
       "2154 -0.062756  0.954737  0.003568    ...    -3.010457 -1.608230 -0.776497   \n",
       "2155  0.313923 -0.196137 -0.139479    ...    -0.620991 -1.045799  0.329202   \n",
       "2156 -0.272191  0.038367 -0.415148    ...    -0.179679  1.171526 -1.369484   \n",
       "2157 -0.030406 -0.185416 -0.305561    ...     0.218312 -0.122961  0.960783   \n",
       "2158  0.664927 -0.363237  0.385481    ...     0.518619  0.745547  1.149459   \n",
       "2159  0.279351 -0.088643 -0.713959    ...    -0.492494 -1.829173  1.729330   \n",
       "2160  0.896142  0.495247 -0.578866    ...     0.165738  0.505194 -1.271706   \n",
       "2161  0.407393 -0.014351 -0.058014    ...     0.233128 -1.048495 -0.312241   \n",
       "2162  1.453737  0.599947 -0.179622    ...     1.546808  0.491209 -1.571067   \n",
       "2163  1.165254  0.407514  0.031691    ...    -0.762699 -0.393018 -3.044596   \n",
       "2164  0.534947  0.343101 -0.271073    ...     0.050476 -0.048363  0.857611   \n",
       "2165 -0.037430  1.085606  0.219417    ...    -0.935748  0.438242 -0.711602   \n",
       "2166  0.129074  0.951555 -0.273831    ...    -0.673105 -0.018660  0.082834   \n",
       "2167  0.925534  0.670530 -0.342585    ...     0.080191  0.140669 -2.010542   \n",
       "2168  0.259379  1.050148 -0.634846    ...     0.647596 -1.174770 -0.029580   \n",
       "2169 -0.933143  1.300051  1.199140    ...     0.245675  0.299113 -0.260392   \n",
       "\n",
       "       ivec594   ivec595   ivec596   ivec597   ivec598   ivec599   ivec600  \n",
       "0    -0.335819  0.070354  0.296477  1.252707  1.078482 -0.801526  0.871624  \n",
       "1     0.509557 -0.864417 -0.295759  0.456390  0.789470  0.396610  0.723979  \n",
       "2     0.649240 -0.143712  0.121022 -0.974557  0.138829 -0.163264 -0.292201  \n",
       "3     0.070033  1.538839  0.406294 -0.334310 -0.633877  0.171371  0.854166  \n",
       "4     0.046693 -0.327969  1.091086 -0.039490  0.975986  1.139612 -0.202748  \n",
       "5     0.092819  0.672037  0.594454  0.236099  0.895553  0.071543  0.121591  \n",
       "6     1.460004  0.566635  0.684433 -0.643174  0.192285 -0.072796  0.381613  \n",
       "7    -0.631255 -0.029913 -0.126166  1.100268 -0.858795  0.039926 -1.009529  \n",
       "8     0.058214 -0.657690  0.468953  0.783177  0.528017  0.952097  0.158702  \n",
       "9    -0.447707 -1.585893  0.333516  1.199462 -0.947476  0.438035  0.547364  \n",
       "10   -0.136060 -0.176556  0.279824 -0.674588 -0.243078  1.580138 -1.699354  \n",
       "11    0.552379 -0.480780  0.584550  0.638610 -0.626269 -1.276253  1.280746  \n",
       "12   -2.082130 -0.328702  0.237509  1.280434  0.309391 -1.045902  0.465395  \n",
       "13    1.853189  0.453691  0.509668  0.144804  0.040649 -0.045122  0.112616  \n",
       "14    0.210066  1.903762 -0.122358 -1.498254 -0.941137  0.295074 -2.267138  \n",
       "15    0.427942 -1.600556  0.246210 -0.795657 -0.036287  1.626201 -3.967272  \n",
       "16   -0.839388  0.789911  0.288444  0.704197  1.187611 -0.508897 -0.913957  \n",
       "17   -0.795963  0.151569  0.965080 -0.090555 -0.161996  0.418260 -0.584478  \n",
       "18   -0.326227  0.920312  0.187032 -1.365379  0.440010  1.876038  0.438103  \n",
       "19   -0.943891  3.092746 -0.567121 -1.521625  0.853881 -0.359619 -1.561343  \n",
       "20    0.976043 -1.173018 -1.164614  1.402002  0.083167  2.412549  0.305316  \n",
       "21    0.283661  0.056439  1.071746  0.246297  0.351041 -0.839595  0.834650  \n",
       "22    0.782344  0.942874  1.157992 -0.466651 -0.227680 -0.666037  0.556310  \n",
       "23   -0.021536 -1.392008  0.152094 -0.007981  0.930795  0.045467 -0.819174  \n",
       "24    1.011852 -1.806159  0.858029  1.840403 -0.265116 -1.386902 -0.277597  \n",
       "25   -0.175708  1.274580 -0.864184  0.406857  0.170143 -0.412133 -0.125966  \n",
       "26   -0.080741 -2.463592  0.786150  1.475682  0.236868  0.049370  1.805464  \n",
       "27    0.006458  0.316867  1.891398  1.753680  0.702816 -0.649200 -0.017082  \n",
       "28   -0.391078 -2.300726  1.532758 -1.082500 -1.473471 -0.341343 -0.015172  \n",
       "29    0.362437 -0.283865 -1.058123 -0.581387 -0.574496  0.260336 -0.392679  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "2140 -0.312507  1.198725 -1.114402  0.656526  1.833696  0.192125  0.819336  \n",
       "2141 -1.239605 -0.313155  0.907743 -0.647262 -0.182566 -0.932861  0.437980  \n",
       "2142 -0.105314 -1.577031 -0.054685  0.896320 -0.252131  0.143151 -0.226842  \n",
       "2143 -0.729582  0.364365  1.305921 -1.335699  0.046512 -1.326234 -0.437820  \n",
       "2144 -0.756522 -1.628930 -0.282916 -0.297052  0.180510 -0.292901  0.210189  \n",
       "2145 -0.026307 -0.363279 -0.243408 -0.733637 -0.517996  0.991660  0.322341  \n",
       "2146 -1.604911 -0.176901 -0.745546  1.627408 -0.233297  0.789341 -0.076185  \n",
       "2147  0.970563  0.601514  0.008572  0.906337 -0.557621 -0.430310  0.745636  \n",
       "2148  2.206533 -0.113028  0.077018 -0.275131  0.605041  1.122494 -0.330695  \n",
       "2149 -0.276782  0.216772  0.070612 -0.562685  0.533959 -0.546800  0.119710  \n",
       "2150 -0.296130  0.173496 -0.862393 -1.533995  0.385015  1.061645 -0.413411  \n",
       "2151 -0.828041  0.573271  1.358465 -0.686744 -0.884768  0.792909  0.052860  \n",
       "2152 -0.370203 -0.411746 -1.200070  1.558526 -0.253793  1.200992  0.497301  \n",
       "2153  0.493684  0.467458 -0.546313  2.608574 -0.632433 -0.155765  1.368509  \n",
       "2154  1.100839 -0.060823  0.371421  0.658277  1.118108  0.102180  2.087003  \n",
       "2155  0.500131  2.004157 -0.669703 -1.128691  0.024542 -0.481156  1.131981  \n",
       "2156 -0.360422  0.117195  0.222658  0.204221  0.662556 -0.200390 -0.253936  \n",
       "2157 -0.210329  0.566338 -1.607740  0.397980  0.091519  0.785763 -0.363546  \n",
       "2158 -0.676299  0.771929  0.260440  1.064718 -0.258190 -1.649790  1.650524  \n",
       "2159 -0.486163  0.775802  0.619463  1.467574 -1.334137 -0.064082  1.900484  \n",
       "2160 -0.233622  0.232578 -0.240881  0.806048 -0.608192  0.729197 -0.403015  \n",
       "2161 -0.090365  1.967659 -0.191724 -0.100377  0.845229  0.518966  0.183473  \n",
       "2162 -0.233093 -0.319234 -1.225319 -0.531166 -0.870199  1.261102  1.573236  \n",
       "2163 -0.017744 -0.326368 -0.645144  0.774340 -0.719842  1.027182 -0.153171  \n",
       "2164  0.084989  0.514933  0.107351  1.172797  0.346443  1.684682 -0.891121  \n",
       "2165  0.908138 -1.687777 -0.721695  0.448478  0.548281  0.277385 -0.809920  \n",
       "2166  1.377634  0.088779  0.524701 -0.539860  0.301450 -0.245628 -0.771783  \n",
       "2167 -0.782719  1.206946  0.014356  1.263567  0.222758  0.822307 -0.692752  \n",
       "2168 -0.774872 -1.400781 -1.683537 -0.668288 -0.500911  1.166515 -2.042598  \n",
       "2169 -0.950292  0.847789  0.745867  0.345427  0.019123 -1.275937  0.668629  \n",
       "\n",
       "[280 rows x 605 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_gender_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'print9' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-6d5d86f05c0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprint9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'print9' is not defined"
     ]
    }
   ],
   "source": [
    "print (print9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
